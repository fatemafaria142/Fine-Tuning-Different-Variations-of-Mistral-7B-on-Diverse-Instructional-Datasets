# Fine-Tuning-of-Mistral-7B-on-Diverse-Instruction-Based-Datasets

## Datasets Used

### Databricks Dolly 15k
- Dataset Link: [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
- Description: The databricks-dolly-15k dataset is an open-source collection of instruction-following records generated by Databricks employees. It encompasses various behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.

### Instruct-v3
- Dataset Link: [instruct-v3](https://huggingface.co/datasets/mosaicml/instruct-v3)
- Description: Instruct-v3 is an aggregate dataset, combining Dolly HHRLHF (derived from Databricks Dolly-15k and Anthropic Helpful and Harmless (HH-RLHF) datasets) with Competition Math, Duorc, CoT GSM8k, Qasper, Quality, Summ Screen FD, and Spider. This dataset aims to provide a permissively-licensed instruction-following dataset with a large number of long-form samples.

## Model

### Mistral-7B-v0.1
- Model Link: [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)
- Description: Mistral-7B-v0.1 is a pretrained generative text model boasting 7 billion parameters. Outperforming Llama 2 13B on all tested benchmarks, it adopts a transformer architecture with innovative features such as Grouped-Query Attention, Sliding-Window Attention, and a Byte-fallback BPE tokenizer.

Feel free to explore the datasets and Mistral-7B-v0.1 model to gain insights into the fine-tuning process on diverse instruction-based datasets and the model's architecture choices.
